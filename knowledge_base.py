#knowledge_base.py
import os
from openai import OpenAI


import hashlib
import shutil
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from utils import split_text, load_googledoc_by_url
from config import KNOWLEDGE_DB_URL, CHUNK_SIZE, CHUNK_OVERLAP

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞
CACHE_DIR = "cache"
os.makedirs(CACHE_DIR, exist_ok=True)  # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç

def get_cache_key(text):
    """–°–æ–∑–¥–∞—ë—Ç —Ö–µ—à-–∫–æ–¥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª—é—á–∞ –∫—ç—à–∞."""
    return hashlib.md5(text.encode()).hexdigest()

def load_from_cache(key):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    cache_path = os.path.join(CACHE_DIR, key)
    if os.path.exists(cache_path):
       
        try:
            return FAISS.load_local(cache_path, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
    else:
        print(f"‚ö†Ô∏è –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω: {cache_path}")
    return None

def save_to_cache(key, vectordb):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à."""
    cache_path = os.path.join(CACHE_DIR, key)
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∫—ç—à, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if os.path.exists(cache_path):
        shutil.rmtree(cache_path)
    
    try:
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –≤ {cache_path}")
        vectordb.save_local(cache_path)
        print(f"‚úÖ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {cache_path}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ FAISS: {e}")

def get_knowledge_base():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ Google Docs, –∫—ç—à–∏—Ä—É–µ—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –µ—ë."""
    text = load_googledoc_by_url(KNOWLEDGE_DB_URL)
    cache_key = get_cache_key(text)
    
    cached_data = load_from_cache(cache_key)
    if cached_data:
       
        return cached_data  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

    print(f"‚ö†Ô∏è –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
    # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
    chunks = split_text(text, CHUNK_SIZE, CHUNK_OVERLAP)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–±–∏–µ–Ω–∏—è
    if not chunks:
        print("‚ùå –û—à–∏–±–∫–∞: —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –≤–µ—Ä–Ω—É–ª–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
        return None

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
    embeddings = OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])

    try:
        vectordb = FAISS.from_documents(chunks, embeddings)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è FAISS: {e}")
        return None

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
    save_to_cache(cache_key, vectordb)
    
    return vectordb